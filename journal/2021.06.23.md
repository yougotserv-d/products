# 2021.06.xx Journal Entry

**Challenge**

Today was mostly spent creating database schema, creating the database tables, and executing an ETL from csv files to the postgres database.

The biggest challenge of the day was working out how to split data that was interrelated from one csv file into three tables. The problem looked like this:

RAW DATA:
features_csv (id, product_id, feature, value)

TABLES:
features (id, feature)
feature_values (id, feature_value)
product_features (id, product_id, feature_id, value_id)

I needed to first create the features and feature_values table from the distinct values in the feature and value column of the csv. Then I needed to turn around and push the unique key of each back to replace the string in the csv, then push it to product_features.

**Actions**
I loaded all the schema and the csv data and created the queries. The queries need refinement, but as they stand now:


INSERT INTO features (feature) (SELECT DISTINCT feature FROM features_csv);

INSERT INTO feature_values (value_name) (SELECT DISTINCT feature_value FROM features_csv);

UPDATE features_csv SET feature_id = (SELECT id FROM features) WHERE feature = (SELECT feature FROM features);

UPDATE features_csv SET feature_value = (SELECT feature_values.id FROM feature_values) WHERE feature_values.value_name = features_csv.feature_value;


These do not yet work as I have not nailed down the query syntax that will allow me to compare the string in a column in one table with the a string from another table, and to then update another column in the original table (because there is a type change from string to integer).

**Result**

I am very close to having all my ETL scripts written, and once I do I am expecting my architecture will allow fast queries.