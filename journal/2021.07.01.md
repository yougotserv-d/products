# 2021.07.01 Journal Entry

**Challenge**

1. Move the database to the EC2 instance.


**Actions**

I created a new instance of EC2 and installed postgres with the following commands:

*** Create the file repository configuration: ***
> sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'

*** Import the repository signing key: ***
> wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -

*** Update the package lists: ***
> sudo apt-get update

*** Install the latest version of PostgreSQL. ***
*** If you want a specific version, use 'postgresql-12' or similar instead of 'postgresql': ***
> sudo apt-get -y install postgresql

After that I needed to create the users for this postgres. The default user for the EC2 instance is 'ubuntu' so I will create a user with that name with create permissions.

I logged into postgresql with the super user:

> sudo -u postgres psql

Then created a new user:

> CREATE USER ubuntu WITH CREATEDB;

I then pushed my database dump file that was created with pg_dump from local storage to the EC2 instance with SCP using the following command line:


> scp -i product.pem /home/alexander/hackreactor/work/yougotserv-d/db.dump ubuntu@172.31.3.205:/home/ubuntu/


I then used pg_restore to recreate the database on the EC2 instance.

> psql -U ubuntu -d postgres < db.dump


**Result**

There was not enough disk space free on the EC2 instance and the process needed to be repeated a few times. One change I made was to change the dump file from a .dump to a .tar, which I then used gzip to compress before pushing it to the EC2 instance. This sped up the SCP immensely.

> gzip db.tar
> scp -i product.pem /home/alexander/hackreactor/work/yougotserv-d/db.tar.gz ubuntu@52.14.129.159:/home/ubuntu/data

I also eventually realized that I still had the csv tables in the database dump, which was probably what was causing the disk space errors.



**Challenge**

Creating the docker image, pushing it to docker hub, and then spinning up a container on the EC2 instance.

**Actions**

I made sure that only the relevant files would be added by updating the dockerignore file, then created the docker image.

> docker build -t alexandercausey/products:latest .

After that I needed to log in to docker hub from my local terminal.

> docker login

Then I was able to push the image to docker hub.

> docker push alexandercausey/products:latest

Over on the EC2 instance I then pulled it down.

> docker pull alexandercausey/products:latest

Then I built the server container.

> docker run -d -p 80:3000 --name server_01 alexandercausey/products:latest

**Result**

The container is up and running as I'd like. Next I'll need to make the database accessible.